# 大模型微调

[TOC]



## 1. 什么是大语言模型中的微调（fine-tuning）？

**答：**
Fine-tuning 是将一个已经预训练好的大型语言模型（LLM）“调整”到特定任务的方法。通过在相关领域的小型数据集上继续训练，模型会微调其参数，使其在该领域表现更佳。预训练阶段已经让模型学习了广泛的语言知识，而微调阶段则利用专业的、聚焦的数据集，让模型掌握特定领域的术语和上下文，提高模型在该任务上的准确性和实用性。简而言之，微调使通用的语言模型变得擅长特定任务，从而获得更加精确和有针对性的输出。



## 2. 请描述为特定任务对预训练模型进行微调的过程。

**答：**
微调预训练模型通常遵循以下步骤：首先准备和清洗数据集，确保数据格式正确、质量良好；必要时可以进行数据增强来提高模型的鲁棒性。接着选择合适的预训练模型，考虑模型大小和与任务相似度。然后设定关键超参数（如学习率、训练轮数、批量大小等），并可能冻结部分底层参数以防过拟合。在训练过程中，要使用验证集监控损失和性能指标（如准确率、精度、召回率等），并根据验证结果不断调整参数和策略。最后，对模型进行部署时要考虑硬件资源、延迟和安全等问题。按照这个结构化流程进行微调，可以逐步提升模型在目标任务上的表现。



## 3. 微调大语言模型有哪些不同的方法？

**答：**
常见的微调方法包括：

* **监督微调（Supervised Fine-Tuning）**：在带标签的数据集上进一步训练模型，模型根据预测误差调整权重，例如用于分类或情感分析等任务。
* **人类反馈强化学习（RLHF）**：结合人类标注的偏好对模型进行微调。先让人类为多种输出排序训练一个奖励模型，然后用强化学习（如PPO）优化语言模型，使其输出更符合人类评价指标。
* **少样本微调（Few-shot Learning）**：当训练数据极少时，仅用少数标注样本训练模型，使其尽可能从有限数据中学习到新任务特性。
* **参数高效微调（PEFT）**：只训练模型的一小部分参数或附加少量新的参数，而冻结其它大部分参数，例如使用 LoRA、前缀调优或提示调优等方法，以节省计算资源。
* **领域自适应预训练（Continual Pre-Training）**：在特定领域的大规模无标注数据上继续预训练（而不是微调），使模型在该领域的语言分布上更通用，然后再进行有监督微调。

这些微调方法各有侧重：监督微调针对具体任务有标注数据；RLHF强调对齐人类意图；PEFT 和少样本方法则关注资源和数据的限制。



## 4. 在微调 LLM 的过程中，什么是灾难性遗忘（catastrophic forgetting）？如何缓解？

**答：**
“灾难性遗忘”指的是模型在对新任务进行微调时，可能会遗忘之前训练任务中学到的知识，导致对旧任务表现大幅下降。比如，将预训练模型在某个领域数据上微调时，模型可能对通用语言能力有所退化。
为减轻遗忘问题，可以：

* 只微调部分参数（如使用 PEFT 方法，只训练少量参数），保留原模型的核心知识；
* 使用弹性权重融合（EWC）等方法，在损失函数中加入约束，限制关键参数的变化；
* 在新旧数据上交替训练（混合训练或记忆重放），确保模型同时关注新旧任务；
* 采用很小的学习率和较少的训练轮数，避免对原始权重的大幅更新。



## 5. 什么是参数高效微调（PEFT），它为什么重要？

**答：**
参数高效微调（PEFT）是一系列在微调时仅调整极少数参数的技术。它重要的原因在于：大型语言模型本身参数量巨大，完全微调成本极高（需要大量显存和计算），而 PEFT 只训练一些附加的小型模块或只更新部分权重，从而节省资源。通过冻结绝大部分原始模型参数，只训练轻量的适配模块，模型仍能学到新任务所需的特性，同时保持原模型的通用知识。PEFT 方法例如 LoRA、IA^3、AdaLoRA 等，可大幅降低微调时显存开销，使得在普通 GPU 上也能处理大模型微调。因此，PEFT 在实际工程中非常流行，有助于降低训练成本和时间。



## 6. 解释低秩适配（LoRA）及其在微调中的工作原理。

**答：**
低秩适配（LoRA）是一种参数高效微调方法，它通过向模型权重矩阵中引入低秩可训练矩阵来调整模型行为，而不改变原有的主要权重。具体来说，LoRA 将原模型的某些权重矩阵（如注意力层的查询/键矩阵）分解为两个小矩阵（低秩近似）并学习它们的更新，原始权重保持冻结。这意味着微调时新增的参数量远小于全量微调，大大节省了显存。LoRA 提出的低秩矩阵能够捕捉任务相关的特征修改，使模型在保持原有参数不变的情况下，学习到适应新任务的能力。总体而言，LoRA 通过这种加小型 Adapter 的方式，实现在不增大模型规模的前提下进行有效微调，并且便于模型共享和部署。



## 7. 什么是量化LoRA（QLoRA）？它与LoRA有何不同？

**答：**
QLoRA 是在 LoRA 基础上结合量化技术的高级微调方法。与 LoRA 相比，QLoRA 在训练模型前将其权重量化（通常使用 4-bit 量化），这样可大幅降低模型的内存占用。在此基础上，再像 LoRA 一样只训练低秩适配矩阵，使得整个微调过程的显存需求更低。QLoRA 引入了诸如双量化（Double Quantization）和分页优化（Paged Optimizers）等技术，在保持或接近原模型性能的前提下，进一步减少内存需求。总的来说，QLoRA 保留了 LoRA 的高效微调思想，同时利用量化使训练更加轻量，可在资源受限的环境中微调较大的模型。



## 8. 比较LoRA、完全微调（full fine-tuning）和前缀/提示微调（prefix/prompt tuning）。

**答：**

* **LoRA（低秩适配）**：只添加并训练少量低秩矩阵，原始参数冻结。相比全量微调，它新增的参数极少，训练内存开销低。适用于只想在现有模型上插入小改动的场景。
* **全量微调（Full Fine-Tuning）**：对模型所有参数进行训练。优点是能充分挖掘模型能力，但需要巨大的计算资源和显存，不适合资源受限环境。
* **前缀/提示调优（Prefix/Prompt Tuning）**：只学习额外的“前缀”或提示嵌入，而不修改模型参数。相比 LoRA，它甚至不改变网络权重，只调整输入形式；因此参数更少，训练更轻量，但在某些任务上效果可能不如 LoRA。

综上，LoRA 介于全量微调和提示调优之间：它不如全量微调灵活但更高效，也比前缀调优能表达更多细节信息。在需要在有限资源下获得较好任务适应性的场景，LoRA 通常是首选。



## 9. 什么是指令微调（instruction fine-tuning，又称为监督微调）？它为何被使用？

**答：**
指令微调（Instruction Tuning 或监督微调，SFT）指的是使用一组包含“用户指令和期望回复”对的数据来训练模型，使其更擅长按照自然语言指令工作。在这个过程中，模型会学习理解输入的指令并生成符合预期的回答。这样做的好处是将通用预训练模型转变为能够更好地“听懂”指令并给出准确回答的助手型模型。指令微调常用于构建聊天机器人、问答系统等对话式应用，在这些场景下模型需要输出连贯且符合人类期望的文本。与普通微调不同，指令微调的数据格式通常为一系列“提示-完成”对，使模型学会从提示中学习任务意图并生成合理结果，从而提高模型与人类交流的有效性。



## 10. 描述人类反馈强化学习（RLHF）在微调 LLM 中的流程。

**答：**
RLHF 是对已有微调模型进行进一步训练的过程，其目标是使模型输出更加符合人类偏好。RLHF 的典型流程如下：首先收集人类对模型输出的反馈数据，把同一个提示下不同输出进行排序。然后训练一个奖励模型，使其能够根据人类的偏好对模型输出进行评分。接下来，使用强化学习算法（如 PPO）更新语言模型的参数，目标是最大化奖励模型给出的分数，这样模型就能生成更符合人类偏好的内容。整个 RLHF 过程保证模型不仅在形式上回答正确，还能在内容上更“有帮助、有诚实、有益”，从而提高生成文本的质量和安全性。



## 11. 何时应选择微调模型而不是使用检索增强生成（RAG）？

**答：**
如果需要模型**学习和记忆**领域知识，并能对特定任务产生专门优化的输出，可以选择微调。微调适用于已有少量标注数据且需要模型深入适应任务的情况，能让模型在该任务上表现出色。而如果主要目的是增强模型对大量外部信息的访问能力，可以选择检索增强生成（RAG）：这时模型保持通用知识，通过检索相关文档补充上下文，适合知识量大、数据更新频繁的场景。一般来说，微调更适用于领域固定且专门问题，RAG 则常用于开放域问答等需要实时检索知识库的任务。



## 12. 缩放规律（scaling laws）如何影响微调决策（模型规模 vs 数据量 vs 计算资源）？

**答：**
Scaling laws 研究了模型性能如何随规模、数据量和算力变化而变化的规律。根据经验，**增大模型规模**往往可以显著提升性能，但也意味着需要更多算力和内存；**增加数据量**或提高数据质量也能提升效果。在微调场景中，这意味着：如果算力充足，可以考虑使用更大的模型进行微调；如果数据量不足，可以适当减小模型规模或使用数据增强；同时要合理分配资源，在模型大小和数据规模之间做平衡。具体到微调策略：对于小数据，可以用较小模型或参数高效方法；对于大模型，则需要用量化、混合精度、梯度检查点等优化技术来节约资源。总之，微调时要根据算力预算和数据情况，遵循提升算力、数据和模型规模之间的平衡原则来做决策。



## 13. 如何准备用于指令微调（如聊天模型）的数据集？

**答：**
为指令微调准备数据集时，需要构建“提示-完成”对。例如，对于聊天模型，数据格式通常是一个输入提示（用户指令或提问）及对应的目标输出（期望的回答）。可以利用已有的 QA 数据、对话日志或人工编写的指令-响应对。数据处理包括清洗无关信息、分句、规范标点，确保输入输出格式与模型需求一致。有时需要对原始对话进行裁剪，或将多轮对话展开为一系列提示-回复对，以适应模型的训练方式。总之，准备过程涉及数据清洗、分段和编码，并确保示例的多样性和代表性，以便模型学习到适当的交互行为。



## 14. 如何使用 HuggingFace Transformers 微调类 GPT 模型？

**答：**
使用 HuggingFace Transformers 微调 GPT 模型的步骤：首先安装依赖和库（Transformers、PEFT 等）。加载预训练模型和相应的 Tokenizer，如 `model = AutoModelForCausalLM.from_pretrained(...)`。准备训练数据集，将文本转换为模型输入格式（例如使用 `tokenizer.batch_encode_plus`）。配置训练超参数（学习率、batch size、轮数等），可以使用 `TrainingArguments`。若使用 PEFT，可先定义 LoRA 配置（`LoraConfig`）并将其添加到模型上。然后使用 `Trainer` 或自定义训练循环：将模型、数据集、损失函数、优化器等传入 Trainer，调用 `trainer.train()` 开始训练。训练完成后，可通过 `model.save_pretrained()` 或 `save_pretrained()` 保存模型（或保存适配器权重）以备部署。整个过程依赖 HF 提供的便捷接口，大大简化了微调流程。



## 15. 选择用于微调的基础模型（如 GPT-2、GPT-3、ChatGPT）时，需考虑哪些因素？

**答：**
选择基座模型时需考虑：

* **模型规模和性能**：较大的模型（如 GPT-3）通常具有更强的语言能力，但训练成本高；较小模型（如 GPT-2）可快速迭代且资源开销低。选择时要平衡性能和资源。
* **预训练数据分布**：模型的预训练数据是否与目标任务相关。某些专用模型可能在特定领域数据上预训练，适合相关任务。
* **输出格式需求**：如果需要对话式输出，可选择经过对话微调的模型（ChatGPT、GPT-NeoX-chat 等）；如只需要单轮生成，可用基础版 GPT。
* **可访问性**：有些大模型可能只能通过 API 调用，无法直接下载权重；而开源模型则可自由微调。

综合来看，应根据任务需求、资源限制和数据域匹配度来选择最合适的基座模型。



## 16. 如何使用 HuggingFace PEFT 库结合 LoRA adapter 进行模型微调？

**答：**
要使用 HuggingFace PEFT 进行 LoRA 微调，首先安装 `peft` 包，然后配置 LoRA 参数并添加到模型中。示例步骤：

1. 导入类：`from peft import LoraConfig, get_peft_model`。
2. 加载预训练模型，如 `model = AutoModelForCausalLM.from_pretrained("gpt2")`。
3. 创建 LoRA 配置：

   ```python
   lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
   ```
4. 将 LoRA 添加到模型：`model = get_peft_model(model, lora_config)` 或使用新版 `model.add_adapter(lora_config, "lora")`。
5. 使用 `Trainer` 进行训练：将上述模型传递给 `Trainer(model=model, ...)`，并调用 `trainer.train()` 开始微调。
   训练完成后，可通过 `model.save_pretrained()` 保存 LoRA 适配器权重。使用 PEFT 时，基础模型权重保持冻结，只有 LoRA 部分的参数被更新，从而实现高效微调。



## 17. 用于评估微调语言模型的指标和方法有哪些？

**答：**
评估微调后的语言模型常用指标包括：

* **困惑度（Perplexity）**：衡量模型对生成任务的预测概率，越低表示模型预测越自信。
* **准确率/精度/召回率**：在分类任务或问答任务中，使用正确率、精度、召回率和 F1 等传统指标衡量输出质量。
* **BLEU/ROUGE**：在机器翻译、摘要等任务中评估生成文本与参考文本的相似度。
* **人类评估**：对于对话或生成任务，常通过人工打分的方式检查回答的连贯性、相关性和自然度。

在实际评价时，通常把验证集输入模型，计算上述指标来衡量微调效果。可视化混淆矩阵、查看典型输出错误也是常用方法，最后还需根据任务特性进行综合判断。



## 18. 如何处理微调时的领域特定词汇或分词问题？

**答：**
对于领域特定的术语或实体，需要考虑词汇表和分词器的适配：

* **扩充词表**：若预训练模型的词汇表不包含某些行业术语，可以向分词器添加新词。HuggingFace 支持用 `tokenizer.add_tokens([...])` 方法增加专有名词，然后扩充模型词嵌入矩阵。
* **重新训练分词器**：对于极为专业的领域，可以在领域语料上重新训练分词器（如 SentencePiece、BPE），从而获得更合适的 token 划分。
* **保留词片段**：如果词汇表无法扩充，也可接受将域名词切分为多个子词（虽然可能降低模型效果，但避免 OOV）。

处理好领域词汇后，再进行微调，模型就能正确理解并生成这些领域相关的术语，提高输出的专业性和准确度。



## 19. 如何在多轮对话数据（如聊天记录）上微调模型？

**答：**
微调多轮对话时，需要将会话历史作为输入的一部分进行训练。常用做法是将对话上下文和当前用户提问拼接在一起作为模型输入，目标输出为接下来的回复。可以使用特殊的分隔符（如“<|user|>”、“<|assistant|>”）标记角色，更清晰地提示模型分辨不同说话者。也可以按照 ChatGPT 风格构建数据：将多轮对话拆分成多个示例，每个示例包含累计的对话上下文和当前回复。然后用监督方法微调模型，使其学会根据对话历史生成连续且有逻辑的回答。这要求准备相应格式的训练数据，并确保数据质量，以充分引导模型理解对话结构。



## 20. 数据增强在微调大语言模型中起什么作用？

**答：**
数据增强可以提高模型的泛化能力，缓解过拟合问题。在语言模型微调中，常见的数据增强方法包括：

* 同义词替换：在输入文本中替换词语为同义词，增加语料多样性。
* 后缀/前缀插入：在句子中随机插入无害的词语或标点，使模型更鲁棒。
* 回译（Back-Translation）：将文本翻译为另一语言再翻译回来，获得新语句。

通过增加样本多样性，模型在见到不同表达时更容易泛化，特别是在标注数据有限的情况下，增强后的数据帮助模型学习到更多不同的语言变体，从而提高稳定性和性能。



## 21. 如何在标注数据有限的情况下有效微调模型？

**答：**
当标注数据稀缺时，可以考虑：

* **使用少样本学习技术**：例如在微调时利用 prompt 学习或少量示例，让模型快速适应新任务。
* **参数高效微调**：像 LoRA 这样的技术可以在小数据上避免过拟合，因为它只更新极少量参数。
* **迁移学习**：从与目标任务相关的任务上继续训练，例如先在大量类似任务数据上微调，再在小数据集上精调。
* **数据增强**：如同义词替换、回译等方法扩充训练数据量。
* **微调目标调低**：使用更小的学习率、增加正则化（如权重衰减、Dropout）和早停策略，减少过拟合风险。

通过结合这些方法，可以在有限数据下依然让模型学到新任务特征，避免彻底过拟合或者遗忘原有知识。



## 22. 如何实现早停（early stopping）或检查点（checkpointing）功能？

**答：**
在使用 HuggingFace `Trainer` 时，可以利用其内置的早停回调和模型保存功能：在 `TrainingArguments` 中设置 `evaluation_strategy="steps"`（或 `"epoch"`）和 `save_strategy`，并启用 `load_best_model_at_end=True` 及 `metric_for_best_model`。然后添加 `EarlyStoppingCallback(early_stopping_patience=…)` 回调，当评估指标在若干步内不再提升时自动停止训练。这样可以自动保存验证集上表现最好的模型并提前终止不再改善的训练。或者在自定义训练循环中，每训练一定步数就在验证集上评估一次，如果指标不再提升则停止，并用 `torch.save()` 保存模型检查点。这些机制有效防止过拟合，并确保最终模型是性能最佳的版本。



## 23. 如何使用 HuggingFace 微调模型以完成问答任务？

**答：**
针对问答任务，常用流程：首先准备数据（如 SQuAD），将每个问题和上下文拼接为输入文本，并生成对应的答案标注。使用 HuggingFace 的 `AutoModelForQuestionAnswering`（或类似任务模型）加载预训练模型和 tokenizer。将数据集转换为模型需要的格式：使用 tokenizer 编码问题和上下文，生成 input\_ids 和 attention\_mask，并将正确的答案 span 转换成 start\_positions/end\_positions。然后使用 `Trainer` 设置训练参数，传入模型和数据集，调用 `trainer.train()` 开始微调。训练过程中监控验证集的准确率和 F1 值。训练结束后，使用 `trainer.predict()` 或 `model(**inputs)` 获取模型在新问题上的预测答案。



## 24. 实际应用中，如何进行 RLHF 流程中的微调？

**答：**
实际应用 RLHF 时，步骤如下：首先需构建一组提示并让模型生成多个不同回答。然后请人工或专家对这些回答按优劣进行排序，形成标注数据。接着训练一个奖励模型，让其学习打分标准（输入提示+回答，输出得分）。最后，将奖励模型与主语言模型结合，用强化学习（如 PPO 算法）更新语言模型参数，使生成的回答尽可能获得高分。训练过程一般采用分布式策略，来处理生成-评分-更新的循环过程。微调结束后，模型的回答会更加贴近人类反馈中高分回答的风格和质量，从而在输出上更“可靠”和“安全”。



## 25. 微调大语言模型时，有哪些防止过拟合的策略？

**答：**
避免过拟合的方法包括：

* **正则化**：应用权重衰减（Weight Decay）惩罚大权重，或在训练时使用 Dropout，限制模型对特定参数过度依赖。
* **冻结部分参数**：只微调模型高层或新增参数，保持其他层不变，减少需要更新的权重数量。
* **适当的超参数**：使用较小学习率、适度的训练轮数、适当的批量大小，以免模型在小数据集上训练过度。
* **早停**：在验证指标停止提升时提前终止训练，避免模型在训练集上过度拟合。
* **数据增强**：通过数据增强生成更多样本，增加训练数据量。
* **交叉验证**：使用不同的数据切分验证模型性能，确保模型在不同验证集上的表现一致。

这些策略可以联合使用，综合提升微调的泛化能力，防止模型只记住训练样本而丧失泛化性。



## 26. 学习率与优化器的选择如何影响微调效果？

**答：**
学习率和优化器对微调过程影响很大：较大的学习率可能导致训练不稳定甚至发散，尤其在对预训练模型进行微调时，因此常使用较低的初始学习率，然后根据训练进度调度（如线性衰减或余弦退火）；较小的学习率则有助于细致调整权重。优化器方面，AdamW 是常用选择，因为它对稀疏梯度和权重衰减处理较好。此外，使用学习率预热（warm-up）阶段可以避免训练初期梯度波动过大，从而让训练更加稳定。总体而言，需要针对任务调优学习率及调度方案，并选择合适的优化器来确保模型稳定收敛。



## 27. 有效缓解灾难性遗忘的方式有哪些？

**答：**
减轻灾难性遗忘的方法有：

* **参数高效方法**：如使用 LoRA、前缀调优等，仅调整少量参数，使模型保留原有知识。
* **弹性权重融合（EWC）**：在损失函数加入约束项，限制与旧任务密切相关参数的改变，保持对旧任务的适应性。
* **混合训练**：使用新旧任务的数据进行联合训练，或定期微调后再“回顾”旧任务数据，保持对旧知识的记忆。
* **渐进式微调**：先训练新任务，再回到旧任务进行小幅度训练，交替进行，防止模型完全偏移到新任务。
  以上方法可以单独或组合使用，让模型在学习新任务的同时尽量保留旧任务的表现。



## 28. 为什么要冻结部分层进行微调？如何操作？

**答：**
冻结参数意味着在微调时保持某些层的权重不变。这样做的好处是：减少需要更新的参数数量，降低显存消耗和计算负担，同时保留预训练模型的通用特征，降低过拟合风险。通常会冻结模型的底层（如嵌入层和部分 Transformer 层），只微调靠近输出的层，这样模型在新任务上获得调整而不会完全丢失原有语言理解能力。在 HuggingFace 中，可以通过设置 `requires_grad=False` 来冻结特定模块，或在 `TrainingArguments` 中使用 `freeze_base_model=True` 等参数来自动冻结底层。冻结策略应根据任务相似度和数据量来定：数据少或任务与预训练差距大时，更倾向冻结更多层。



## 29. 批量大小（batch size）和梯度累积（gradient accumulation）对大模型微调有何影响？

**答：**
批大小（batch size）与模型训练效果和资源使用密切相关：较大的批量通常使训练更稳定，但受限于 GPU 显存。在显存不足的情况下，可以使用梯度累积（gradient accumulation）：将多个小批量梯度累加后再进行一次参数更新，相当于使用更大批量。这样既能稳定训练，又不会超出显存限制。例如设置 `gradient_accumulation_steps=4`，就可以用4个小批量模拟一个大批量。梯度累积有助于在大模型训练时使用更有效的批量大小，而不牺牲性能。



## 30. 哪些正则化技术（如 dropout、权重衰减）有助于微调？

**答：**
正则化方法可以防止模型过拟合：

* **权重衰减（Weight Decay）**：在优化过程中对权重值添加惩罚项，使权重保持较小，避免过度拟合。
* **Dropout**：在训练时随机丢弃部分神经元输出，让模型不依赖特定特征，提高泛化能力。适当的 Dropout 比例（如 0.1）在微调时也常用。
* **Gradient Clipping**：限制梯度范数，防止梯度爆炸，使训练更稳定。
* **Early Stopping**：在验证损失不再下降时停止训练，也是一种正则化策略。

这些技术可以配合使用，例如在 `TrainingArguments` 中设置 `weight_decay` 和 `adam_epsilon`。权重衰减帮助限制大权重带来的过拟合风险，而 Dropout 则减少了对特定路径的依赖，从而提高了模型在新数据上的鲁棒性。



## 31. 为什么在大模型微调中要使用梯度累积或检查点？

**答：**
对大模型进行微调时，梯度累积和梯度检查点（gradient checkpointing）是常用的内存优化技术：

* **梯度累积**：允许使用较小的实际批量多次前向/反向传播后累积梯度，再更新一次权重。这样可以在有限显存下模拟大批量训练，提高稳定性。
* **梯度检查点**：在前向传播时不保留中间激活，通过在反向传播时重新计算部分前向计算来节省显存。对模型进行更多前向/后向计算换取显存节省，适用于显存紧张的大模型微调。

使用这些技术可以显著降低显存需求，使得大型模型在单卡或较小资源上可训练，同时保持合理的批量大小和训练效果。



## 32. 微调 LLM 时有哪些内存和计算优化方法？

**答：**
微调大型模型时常用的优化手段包括：

* **混合精度训练（FP16）**：使用半精度浮点数减少显存占用，并加速计算。大多数框架（如 PyTorch）提供自动混合精度（AMP）支持。
* **模型分布式/并行**：使用多卡并行（数据并行、模型并行或流水线并行）来分摊模型和数据，提高效率。
* **量化训练**：在训练时将模型参数量化（如 4-bit），如 QLoRA 就是此类技术，能显著降低内存使用。
* **去激活梯度**：只对需要更新的参数计算梯度（例如 Frozen 层不计算梯度）。
* **使用优化库**：像 DeepSpeed 或 Accelerate 等库集成了多种优化策略（如零冗余优化 ZeRO、CPU/GPU 内存分页技术）来降低内存开销。

这些优化策略通常可以组合使用，以保证在有限硬件条件下完成大模型的微调任务。



## 33. 学习率预热（warmup）和学习率调度如何帮助稳定微调过程？

**答：**
学习率预热（Warmup）和调度策略可以使训练更加稳定：预热阶段先用非常小的学习率逐渐增大至设定值，避免训练初期梯度更新过大导致的发散问题；完成预热后，再逐步降低学习率（如线性或余弦衰减），让模型在训练后期更细致地收敛。合理的学习率调度能在不同训练阶段给予模型合适的学习速率，提高训练效果和稳定性。此外，使用自适应优化器（如 AdamW）结合调度器也是常见做法，以更好地控制权重更新过程。



## 34. 持续预训练（continual pre-training）与任务微调（task fine-tuning）有何区别？

**答：**

* **Continual Pre-Training**：在无监督的大规模语料上继续训练模型，使其学习新的领域通用特征，但不使用特定任务的标签。通常用于让模型更好地掌握某个领域的语言分布后，再进行微调。
* **任务微调**：在有监督的标注数据上训练模型，使其掌握具体任务的映射关系。微调阶段直接使用任务的输入-输出对，指导模型执行目标任务。

简单来说，Continual Pre-Training 主要更新词表和语言表示层（学习领域知识），而 Fine-Tuning 则是在此基础上进一步学习任务相关映射（如分类或生成规则）。二者可以结合使用：先在领域数据上预训练，再在任务数据上微调，从而获得更好的性能。



## 35. 量化或模型蒸馏如何与微调 LLM 相关？

**答：**
量化和蒸馏是两种常用的模型压缩技术：

* **量化（Quantization）**：将模型权重和/或激活值从高精度（如 FP32）转换为低精度（如 8-bit、4-bit）。与微调结合时，可以在量化后的模型上做微调（如 QLoRA），进一步减小显存占用，使训练和推理更高效，同时通常还能保持接近原精度的性能。
* **知识蒸馏（Distillation）**：将一个大模型（Teacher）训练出的知识转移给一个小模型（Student）。在微调场景下，可以先用大模型生成对新任务的预测，然后用这些预测“指导”小模型训练，使小模型在保留性能的同时减少参数量。

这两者都可以视为微调后的后续优化步骤：量化使训练部署更轻量，蒸馏则在性能-体积权衡上取得平衡。它们通常与微调过程结合，使 LLM 在实际部署中更高效。



## 36. 如何进行 LLM 微调任务的超参数调优？

**答：**
超参数调优通常包括网格搜索或贝叶斯优化：针对重要参数（学习率、批量大小、epoch 数、层冻结策略等）设置多个候选值或范围。使用训练验证循环，在验证集上评估不同组合的性能，选择最佳超参数。HuggingFace 提供了 `TrainerHyperTrainer` 或集成了 Optuna 等优化器；Autotrain 平台则可自动搜索。关键是合理选择调参目标（如验证损失或准确率）和搜索范围，有时结合经验法则（如先固定其它参数只优化学习率和权重衰减），逐步缩小搜索空间，找到令微调效果最佳的参数配置。



## 37. 微调时如何解决数据不平衡问题？

**答：**
数据不平衡时可以：

* **调整损失权重**：给少数类样本更高的损失权重，使模型更关注这些类；
* **过采样/欠采样**：对少数类进行数据增强或重复，或下采样多数类，使类别分布更均衡；
* **使用适合不平衡的指标**：比如 F1 分数而非准确率来评估，从而更关注少数类性能；
* **集成方法**：训练多个模型或采用集成算法来减少偏向。

在微调 LLM 时，如果是分类任务，可通过上述策略增加少数类别的样本数或权重来提高模型对其的敏感度，从而缓解预测偏差。



## 38. 什么是弹性权重固定（Elastic Weight Consolidation），它如何帮助微调？

**答：**
弹性权重融合（Elastic Weight Consolidation, EWC）是一种避免遗忘的方法。在多任务学习或连续微调中，EWC 在损失函数中引入一个项，用于约束模型关键参数偏离上一个任务的值，保留其旧任务的表现。具体做法是计算每个参数对旧任务的重要性（如 Fisher 信息矩阵），并增加一个惩罚项，使那些重要参数在新任务上微调时不能大幅变化。这样在学习新任务时，同时保留了对旧任务的能力。EWC 可与 LLM 的微调结合，防止模型在新领域训练时彻底丧失先前知识，从而减轻灾难性遗忘。



## 39. 编写代码：使用 LoRA 和 HuggingFace PEFT 微调 Transformer 模型。

**答：**

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

# 加载预训练模型
model = AutoModelForCausalLM.from_pretrained("gpt2")
# 创建 LoRA 配置
lora_config = LoraConfig(
    task_type="CAUSAL_LM", inference_mode=False,
    r=8, lora_alpha=32, lora_dropout=0.1
)
# 将 LoRA 添加到模型
model = get_peft_model(model, lora_config)

# 定义训练参数
training_args = TrainingArguments(
    output_dir="./lora_model", num_train_epochs=3, per_device_train_batch_size=4,
    learning_rate=3e-5, save_steps=1000
)
trainer = Trainer(model=model, args=training_args, train_dataset=my_dataset)

# 开始训练
trainer.train()
```

此代码示例中，我们使用 `LoraConfig` 将 LoRA 适配器添加到 GPT-2 模型，并通过 HuggingFace 的 `Trainer` 进行微调。注意 `inference_mode=False` 表示正在训练模式；训练完成后，可用 `model.save_pretrained()` 保存适配器权重。



## 40. 编写代码：冻结 Transformer 模型中除最后分类层外的所有层。

**答：**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
# 冻结所有层
for param in model.base_model.parameters():
    param.requires_grad = False
# 解冻分类头
for param in model.classifier.parameters():
    param.requires_grad = True
```

这段代码示例中，`model.base_model` 对象包含了 BERT 的所有 Transformer 层，将它们的 `requires_grad` 设为 `False` 即可冻结。之后我们只对模型的 `classifier`（分类头）进行训练，使微调时仅更新最后一层的参数。



## 41. 编写代码：在 PyTorch 训练循环中集成梯度检查点（gradient checkpointing）。

**答：**

```python
model = AutoModelForCausalLM.from_pretrained("gpt2")
model.gradient_checkpointing_enable()  # 启用梯度检查点
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

for epoch in range(num_epochs):
    for batch in dataloader:
        inputs = batch['input_ids'].to(device)
        outputs = model(inputs, labels=inputs)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

此代码通过调用 `model.gradient_checkpointing_enable()` 在 HuggingFace 模型上开启了梯度检查点功能（在后向传播时重新计算前向过程以节省显存）。然后使用常规训练循环进行前向和反向计算。使用梯度检查点可以在训练大型模型时显著减少内存占用。



## 42. 编写代码：在验证数据上计算语言模型的困惑度（perplexity）。

**答：**

```python
import math
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model.eval()

valid_texts = ["This is a sample sentence.", ...]  # 验证集样本
total_loss = 0
total_tokens = 0

with torch.no_grad():
    for text in valid_texts:
        inputs = tokenizer(text, return_tensors="pt").to(device)
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss.item()
        total_loss += loss * inputs["input_ids"].size(1)
        total_tokens += inputs["input_ids"].size(1)

avg_loss = total_loss / total_tokens
perplexity = math.exp(avg_loss)
print("Perplexity:", perplexity)
```

这段代码使用 GPT-2 模型计算平均损失，然后通过 `perplexity = exp(loss)` 得到困惑度（Perplexity）。困惑度可以评价语言模型对验证集的拟合程度。



## 43. 使用 HuggingFace PEFT，展示如何添加 LoRA adapter 并对模型进行训练。

**答：**

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig

model = AutoModelForCausalLM.from_pretrained("gpt2")
lora_config = LoraConfig(
    task_type="CAUSAL_LM", r=4, lora_alpha=16, lora_dropout=0.05
)
model.add_adapter(lora_config, adapter_name="lora")
model.train_adapter("lora")

training_args = TrainingArguments(
    output_dir="./lora", num_train_epochs=2, per_device_train_batch_size=8
)
trainer = Trainer(model=model, args=training_args, train_dataset=my_dataset)
trainer.train()
```

在这个例子中，我们先用 `add_adapter` 给模型添加一个名为 “lora” 的 LoRA 适配器，并调用 `train_adapter("lora")` 使之进入训练模式。然后用 `Trainer` 对带有 LoRA 适配器的模型进行微调。训练结束后，可用 `model.save_adapter("./lora")` 保存适配器权重。



## 44. 展示如何在基于 HuggingFace Trainer 的微调中实现早停。

**答：**

```python
from transformers import TrainerCallback

class EarlyStopCallback(TrainerCallback):
    def __init__(self, early_stopping_patience=3):
        self.patience = early_stopping_patience
        self.best_loss = float("inf")
        self.counter = 0

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        loss = metrics.get("eval_loss")
        if loss is None:
            return
        if loss < self.best_loss:
            self.best_loss = loss
            self.counter = 0
        else:
            self.counter += 1
        if self.counter >= self.patience:
            control.should_training_stop = True

# 使用示例
trainer = Trainer(
    model=model, args=training_args, 
    train_dataset=train_ds, eval_dataset=eval_ds,
    callbacks=[EarlyStopCallback(early_stopping_patience=2)]
)
trainer.train()
```

此代码创建了一个自定义回调类，在每次评估后检查验证损失，如果连续 `patience` 次没有提升，则设置 `control.should_training_stop = True` 停止训练。将该回调传给 `Trainer` 后，就实现了早停功能。



## 45. 编写代码：微调后仅保存 adapter 权重。

**答：**

```python
# 假设 model 已经附加并训练了 LoRA 适配器，命名为 "lora"
model.save_adapter("./adapter_folder", "lora")
```

使用 HuggingFace PEFT 时，调用 `model.save_adapter(save_directory, adapter_name)` 可仅保存特定适配器的权重和配置，而不保存整个基座模型。这样便于分享或在推理时只加载微调部分。



## 46. 编写代码：将 LoRA adapter 的权重合并回基础模型。

**答：**

```python
from peft import PeftModel

# 加载基座模型和 LoRA 适配器
base_model = AutoModelForCausalLM.from_pretrained("gpt2")
peft_model = PeftModel.from_pretrained(base_model, "./adapter_folder/lora")
# 将适配器权重融合进基座模型
merged_model = peft_model.merge_and_unload()
merged_model.save_pretrained("./merged_model")
```

这里使用 `peft.PeftModel` 加载了 LoRA 模型，然后调用 `merge_and_unload()` 方法将 LoRA 权重合并回基座模型，得到一个包含微调效果的常规模型。最后保存该合并后的模型。



## 47. 编写代码：将模型量化为 4-bit 并按 QLoRA 方法微调。

**答：**

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)

model = AutoModelForCausalLM.from_pretrained(
    "gpt2", quantization_config=bnb_config
)
lora_config = LoraConfig(task_type="CAUSAL_LM", r=4, lora_alpha=16, lora_dropout=0.05)
model = get_peft_model(model, lora_config)

training_args = TrainingArguments(output_dir="./qlo ra", num_train_epochs=2)
trainer = Trainer(model=model, args=training_args, train_dataset=my_dataset)
trainer.train()
```

此示例中，我们使用 `BitsAndBytesConfig` 将模型加载为 4-bit 量化格式，然后再添加 LoRA 并训练。这样结合了量化和 LoRA，即 QLoRA 方法，可以在大幅减少内存使用的同时完成微调。



## 48. 编写代码：准备用于微调的指令跟随（instruction-following）示例的 JSON 数据集。

**答：**

```python
import json

data = []
with open("raw_data.txt") as f:
    for line in f:
        prompt, response = line.strip().split("\t")
        data.append({
            "prompt": prompt,
            "completion": response
        })

with open("instr_dataset.json", "w") as outfile:
    json.dump(data, outfile, ensure_ascii=False, indent=2)
```

此代码示例将原始的“提示\t回答”数据转换为 JSON 格式的指令微调数据集。每个条目包含 `prompt` 和 `completion` 字段，符合 HuggingFace 等库对指令数据的期望格式。生成的 `instr_dataset.json` 即可用于后续的 Fine-tuning 数据加载。



